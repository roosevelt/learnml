{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradiente Descendente</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Provavelmente um dos mais populares métodos de otimização de parâmetros em aprendizado de máquina, \n",
    "o **Gradiente Descendente** nos permite encontrar, de forma iterativa, pontos que levam a valores mínimos de funções. Modelamos funções que representam erros de classificação ou predição, e por meio da aplicação deste método, descobrimos parâmetros que possibilitam minimizá-los. Muitas vezes, por meio da otimização de parâmetros dessas funções que modelam erros e custos é que se torna possível realizar o aprendizado de máquina. É pela transformação de nossos problemas de **Machine Learning** em problemas de otimização de parâmetros que podemos aplicar métodos como o Gradiente Descendente e resolvê-los sem maiores complicações.</div>\n",
    "<br>\n",
    "\n",
    "<div align=\"justify\">O método do Gradiente Descendente é muito utilizado para o aprendizado em **Redes Neurais Artificiais**, modelo que fornece as bases para o aprendizado das técnicas de **Deep Learning**, tão populares atualmente. Um dos motivos da grande utilização da aprendizagem profunda, como também é conhecida, é o seu bom desempenho em problemas como processamento de imagens, processamento de linguagem natural, mineração de dados e de texto, criação automática de filmes, imagens e de música, entre outras tantas aplicações.</div>\n",
    "<br>\n",
    "\n",
    "<div align=\"justify\">Como veremos, a implementação em uma linguagem de programação do Gradiente Descendente é muito simples. Além disso, também é trivial compreender as principais idéias que permitem a sua aplicação. Entretanto, para que você compreenda bem as bases matemáticas que levam ao método é importante conhecer as técnicas de derivação de funções de mais de uma variável. Não se preocupe, mesmo que você não domine a parte matemática será possível compreender os conceitos principais, e isto será suficiente para aplicar o modelo a problemas diversos.</div>\n",
    "<br>\n",
    "\n",
    "<div align=\"justify\">Neste artigo, explicaremos o funcionamento do algoritmo do Gradiente Descendente, sua fundamentação matemática, a implementação em código (Python) do método e algumas aplicações. É importante que você possua familiaridade com o desenvolvimento de software em Python para melhor compreensão das implementações.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Então, o que é o tal gradiente?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Considere uma função $\\:f(x,y):\\mathbb{R}^2\\longrightarrow\\mathbb{R}$. Sabemos que a derivada direcional desta função na direção do vetor unitário $\\overrightarrow{\\bf u}(u_1,u_2)$ é dada pela equação abaixo. A derivada direcional nos mostra a taxa de crescimento ou decrescimento de uma função em uma dada direção, neste caso, a direção é a do vetor unitário $\\overrightarrow{\\bf u}$.</div>\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "$\\dfrac{df(x,y)}{d\\overrightarrow{\\bf u}} = \\dfrac{\\partial f(x,y)}{\\partial x} u_1 + \\dfrac{\\partial f(x,y)}{\\partial y} u_2 $\n",
    "</div>\n",
    "<br>\n",
    "<div align=\"justify\">\n",
    "Pela definição do gradiente, escrevemos também:\n",
    "<div>\n",
    "<div align=\"center\">\n",
    "    $\\nabla f = \\left(\\dfrac{\\partial f(x,y)}{\\partial x},\\dfrac{\\partial f(x,y)}{\\partial y}\\right)$\n",
    "</div>\n",
    "<br>\n",
    "<div align=\"justify\">\n",
    "Consequentemente, podemos reescrever a derivada direcional como produto interno do gradiente pelo vetor direção:\n",
    "<div>\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    $\\dfrac{df(x,y)}{d\\overrightarrow{\\bf u}} = \\nabla f \\bullet \\overrightarrow{\\bf u} = ||\\nabla f||\\: || \\overrightarrow{\\bf u} || \\cos{\\theta}$\n",
    "</div>\n",
    "<br>\n",
    "<div align=\"justify\">\n",
    "Como o vetor u é unitário, $||\\overrightarrow{\\bf u}||=1$ e desta forma: $\\dfrac{df(x,y)}{d\\overrightarrow{\\bf u}} = ||\\nabla f|| \\cos{\\theta}$. <div>\n",
    "\n",
    "<br>\n",
    "<div align=\"justify\">\n",
    "Analisando está fórmula, vemos que o valor máximo da derivada parcial é $||\\nabla f||$, pois a função $\\cos{\\theta}$ atinge seu valor máximo em 1. Além disso, atinge seu mínimo em -1 e portanto a derivada parcial atinge seu mínimo em $-||\\nabla f||$. É importante observarmos que se assumimos que $\\cos \\theta = 1$, estamos considerando também que o vetor gradiente possui a mesma direção do vetor unitário. Então, podemos concluir que é na direção e sentido do vetor gradiente que a derivada direcional percebe seu valor máximo, ou seja, na direção e sentido do vetor gradiente é que a função cresce mais rapidamente. Da mesma forma, na mesma direção, porém em sentido contrário é para onde decresce mais rapidamente.<div>\n",
    "<br>\n",
    "<div align=\"justify\">\n",
    "Estas informações sobre o crescimento ou decrescimento da função são fundamentais para compreendermos o algoritmo do gradiente descendente e, analogamente, o mesmo raciocínio poderia ser aplicado para compreender o gradiente ascendente.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
